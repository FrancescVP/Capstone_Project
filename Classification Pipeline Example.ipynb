{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd00f9cfdd72e9992f909e95dd59f40a3418d08e1877421f3ae0567c38eb6c8a49a",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "607dab78e5b01418678e2ed452cc71b204d428fef5264ef64884a8608f31e7a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# **Libraries**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load Code\n",
    "from load_data import data_loader\n",
    "\n",
    "# Preprocessing functions\n",
    "from preprocessing import classifier_pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, KFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, mean_squared_error, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "source": [
    "# **Load the data**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\franc\\OneDrive\\Documentos\\ProgramaciÃ³\\Data Science\\Projects\\Capstone_Project\\load_data.py:60: FutureWarning: Your version of xlrd is 1.2.0. In xlrd >= 2.0, only the xls format is supported. As a result, the openpyxl engine will be used if it is installed and the engine argument is not specified. Install openpyxl instead.\n  clinical_data = pd.read_excel(path + \"subject_clinical_data.xlsx\")\n"
     ]
    }
   ],
   "source": [
    "full_dataset, fa, func, gm = data_loader(unzip=False)"
   ]
  },
  {
   "source": [
    "# **Classification Pipeline**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[neuroCombat] Creating design matrix\n",
      "[neuroCombat] Standardizing data across features\n",
      "[neuroCombat] Fitting L/S model and finding priors\n",
      "[neuroCombat] Finding parametric adjustments\n",
      "[neuroCombat] Final adjustment of data\n",
      "Statistically diferences in 1132 of connections\n",
      "Statistically diferences in 185 of connections with FDR\n",
      "Statistically diferences in 381 of connections\n",
      "Statistically diferences in 6 of connections with FDR\n",
      "Statistically diferences in 130 of connections\n",
      "Statistically diferences in 4 of connections with FDR\n"
     ]
    }
   ],
   "source": [
    "X, y = classifier_pipeline(fa, gm, func)"
   ]
  },
  {
   "source": [
    "# **Some ML Models**\n",
    "\n",
    "## 1. Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9631795820064148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def evaluation(X, y, model):\n",
    "    mean_score = 0\n",
    "    logistic_model = model\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "    sc = StandardScaler()\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, y_train = sc.fit_transform(X.iloc[train_index, :]), y.iloc[train_index]\n",
    "        X_test, y_test = sc.transform(X.iloc[test_index, :]), y.iloc[test_index]\n",
    "        logistic_model.fit(X_train, y_train)\n",
    "        y_hat = logistic_model.predict(X_test)\n",
    "        mean_score += f1_score(y_hat, y_test)\n",
    "    print(mean_score/5)\n",
    "\n",
    "evaluation(X, y, LogisticRegression())"
   ]
  },
  {
   "source": [
    "## 2. Suport Vector Regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_std = sc.fit_transform(X)\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf', 'linear', 'poly', 'sigmoid'], \n",
    "                     'gamma': [1e-3, 1e-4, 1e-2, 1e-5],\n",
    "                     'C': [1, 10, 100, 500, 1000, 3000]}]\n",
    "\n",
    "clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='f1')\n",
    "\n",
    "clf.fit(X_std, y)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9700761337305529\n"
     ]
    }
   ],
   "source": [
    "evaluation(X, y, SVC(kernel=\"rbf\", C=15, gamma=0.001))"
   ]
  },
  {
   "source": [
    "## 3. Random Forest Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 200}"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 500, 700, 1000],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [2, 3, 5, 7]\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X_std, y)\n",
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.967251461988304\n"
     ]
    }
   ],
   "source": [
    "evaluation(X, y, RandomForestClassifier(max_depth=13, max_features = \"sqrt\", n_estimators = 200))"
   ]
  },
  {
   "source": [
    "## 4. XGBoost Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'colsample_bytree': 0.5, 'gamma': 0.2, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_std = sc.fit_transform(X)\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{\"learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "                     \"max_depth\": [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "                     \"min_child_weight\": [ 1, 3, 5, 7 ],\n",
    "                     \"gamma\": [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "                     \"colsample_bytree\": [ 0.3, 0.4, 0.5 , 0.7 ] }]\n",
    "\n",
    "xgb_gs = GridSearchCV(\n",
    "        XGBClassifier(), tuned_parameters, scoring='f1')\n",
    "\n",
    "xgb_gs.fit(X_std, y)\n",
    "print(xgb_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9785714285714286\n"
     ]
    }
   ],
   "source": [
    "evaluation(X, y, XGBClassifier(colsample_bytree= 0.5, gamma= 0.2, learning_rate= 0.05, max_depth= 3, min_child_weight= 5))"
   ]
  }
 ]
}